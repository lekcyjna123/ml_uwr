{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janchorowski/ml_uwr/blob/fall2020/Homework3/Homework3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkKY6us_cCg4"
      },
      "source": [
        "# Homework 3\n",
        "\n",
        "**For exercises in the week 7-11.12.20**\n",
        "\n",
        "**Points: 7 + 1 bonus point**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiZk6ikjhsVL"
      },
      "source": [
        "## Problem 1 [1.5p]\n",
        "\n",
        "Consider AdaBoost.\n",
        "Prove that $\\alpha_t = \\frac{1}{2}\\log\\frac{1-\\epsilon_t}{\\epsilon_t}$, where $\\epsilon_t$ is the error rate is the optimal coefficient for the $t$-th weak learner under the exponential loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB9QqTlKFLH9"
      },
      "source": [
        "## Problem 2 [0.5p]\n",
        "\n",
        "Would it make sense to apply boosting in a regression setting using linear regression as the base learner?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ptwFU57fNeR"
      },
      "source": [
        "## Problem 3 [0.5p]\n",
        "\n",
        "Suppose you apply AdaBoost using fully grown trees as the base learner. What would happen? What would the final ensemble look like?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S969Z-q408Sn"
      },
      "source": [
        "## Problem 4 [1.5p]\n",
        "\n",
        "You want to apply boosting to least squares regression. You use a classifier as the weak learner, it learns functions that map any $x$ to $\\pm1$. Prove that during each boosting round the optimal weak learner is the one whose output is maximally correlated with the residuals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk8vTUfxxs3M"
      },
      "source": [
        "## Problem 5 [Bishop 14.7] [1p]\n",
        "\n",
        "Consider a datasample $x$ with label distribution $p(y|x)$ (thus we assume that the sample may be ambiguous). Assume the boosted ensemble returns for this sample a score $f_x$.\n",
        "\n",
        "The expected loss on this sample is \n",
        "$$\n",
        "\\mathbb{E}_{y|x}\\left[e^{-yf_x}\\right] = \\sum_{y\\in\\pm 1}e^{-yf_x}p(y|x)\n",
        "$$\n",
        "\n",
        "Find the value of the model output $f_x$ which minimizes $\\mathbb{E}_{y|x}\\left[e^{-yf_x}\\right]$ and express it in terms of $p(y=1|x)$ and $p(y=-1|x) = 1 - p(y=1|x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_Nd4O2PP5mq"
      },
      "source": [
        "## Problem 6 [0.5p]\n",
        "\n",
        "Consider a dataset with 400 examples of class C1 and 400 of class C2. \n",
        "Let tree A have 2 leaves with class distributions:\n",
        "\n",
        "| Tree A   | C1    | C2  |\n",
        "|----------|-------|-----|\n",
        "| Leaf 1   | 100   | 300 |\n",
        "| Leaf 2   | 300   | 100 |\n",
        "\n",
        "and let tree B have 2 leaves with class distribution:\n",
        "\n",
        "| Tree B   | C1    | C2  |\n",
        "|----------|-------|-----|\n",
        "| Leaf 1   | 200   | 400 |\n",
        "| Leaf 2   | 200   |   0 |\n",
        "\n",
        "What is the misclassification rate for both trees? Which tree is more pure according to Gini or Infogain?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVFSEXy_g_fo"
      },
      "source": [
        "## Problem 7 [0.5p]\n",
        "\n",
        "Suppose you work on a binary classification problem and train 3 weak classifiers. You combine their prediction by voting. \n",
        "\n",
        "Can the training error rate of the voting ensemble smaller that the error rate of the individual weak predictors? Can it be larger? Show an example or prove infeasibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v45ymTuqwfz-"
      },
      "source": [
        "## Problem 8 [Bishop 14.5] [1p]\n",
        "\n",
        "Consider an ensemble model in which we allow unequal weights for the constituing models, so that:\n",
        "\n",
        "$$\n",
        "F(x) = \\sum_{t=1}^T \\alpha_t f_t(x).\n",
        "$$\n",
        "\n",
        "We want to ensure that the value of $F$ is bounded by the maximal and minimal output of the individual models, that is:\n",
        "$$\n",
        "\\min_t f_t(x) \\leq F(X) \\leq \\max_t f_t(x).\n",
        "$$\n",
        "\n",
        "Show that a necessary and sufficient condition for this constraint is that the coefficients satisfy:\n",
        "$$\n",
        "\\alpha_t \\geq 0\\quad\\text{ and }\\quad \\sum_t\\alpha_t = 1.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMsfyd6FizvB"
      },
      "source": [
        "## Problem 9 [Bishop 14.8] [1bp]\n",
        "\n",
        "Show that the exponential error loss, which is minimized by the AdaBoost algorithm, does not correspond to the log likelihood of any well-behaved probabilistic model. This can be done by showing that the corresponding conditional distribution $p(y|x)$ cannot be correctly normalized."
      ]
    }
  ]
}