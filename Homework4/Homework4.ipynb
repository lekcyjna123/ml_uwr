{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janchorowski/ml_uwr/blob/fall2020/Homework4/Homework4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3gmko6zus4J"
      },
      "source": [
        "# Homework 4\n",
        "\n",
        "**For exercises between 20-21.01.2021**\n",
        "**The bonus problem can be turned in until the last day of semester**\n",
        "\n",
        "**Points: 9 + 4bp**\n",
        "\n",
        "\n",
        "$\\def\\R{{\\mathbb R}} \\def\\i{^{(i)}} \\def\\sjt{\\mathrm{s.t. }\\ }$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfOp2jR-l6Ro"
      },
      "source": [
        "# Problem 1 (Bishop) [1.5p]\n",
        "\n",
        "Consider $K$-element mixtures of $D$-dimensional binary vectors. Each component of the mixture uses a different Bernoulli distribution for each dimension of the vector:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "p(z=k) &= \\pi_k \\quad \\text{with } 0 \\leq \\pi_k \\leq 1 \\text{ and } \\sum_k\\pi_k = 1\\\\\n",
        "p(x | z=k) &= \\prod_{d=1}^{D} \\mu_{kd}^{x_d}(1-\\mu_{kd})^{(1-x_d)}\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "where $x\\in\\{0,1\\}^D$ is a random vector. The $k$-th mixture component is parameterized by $D$ different probabilities $\\mu_{kd}$ of $x_d$ being 1.\n",
        "\n",
        "Do the following:\n",
        "- Write an expression for the likelihood ($p(x;\\pi,\\mu)$).\n",
        "- Compute the expected value of $x$.\n",
        "- Compute the covariance of $x$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4TIhWQGl5p8"
      },
      "source": [
        "# Problem 2 [2bp]\n",
        "\n",
        "Derive an E-M scheme for fitting a mixture of Bernoulli distributions as defined in Problem 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TNgwkXtvJLT"
      },
      "source": [
        "# Problem 3 [1.5p]\n",
        "\n",
        "Let $X\\in \\R^{D\\times N}$ be a data matrix containing $N$ $D$-dimensional points. Furthermore assume $X$ is centered, i.e. \n",
        "$$\n",
        "\\sum_{n=1}^N X_{d,n} = 0 \\quad \\forall d.\n",
        "$$\n",
        "\n",
        "Read about the SVD matrix decomposition (https://en.wikipedia.org/wiki/Singular_value_decomposition). \n",
        "\n",
        "Show:\n",
        "- **P3.1** [0.5p] how the singular vectors of $X$ relate to eigenvectors of $XX^T$\n",
        "- **P3.2** [1p] that PCA can be interpreted as a matrix factorization method, which finds a linear projection data which retains the most information about $X$ (in the least squares sense)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6rmavsV2ifp"
      },
      "source": [
        "# Problem 4 [1p]\n",
        "\n",
        "Consider orthonormal matrices whose entries are non-negative. What can they express?\n",
        "\n",
        "What would be the limitations of learning an NMF factorization when the columns of the  $W$ matrix (defined below) are orthogonal?\n",
        "\n",
        "NMF definition:\n",
        "$$\n",
        "X \\approx W\\cdot H\n",
        "$$\n",
        "in which $X\\in \\mathbb{R^+}^{D \\times N}$ is the data matrix containing $N$ examples in $D$ dimensions, $W\\in \\mathbb{R^+}^{D \\times K}$ is the dictionary of NMF features and $H \\in \\mathbb{R^+}^{K \\times N}$ gives the encoding of each data sample, and $\\mathbb{R^+}$ is the set of non-negative real numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttRHmBU8tFbU"
      },
      "source": [
        "# Problem 5 [2p]\r\n",
        "\r\n",
        "Consider a symmetric real matrix A, i.e. $A\\in\\R^{D\\times D}$ and $A=A^T$.\r\n",
        "\r\n",
        "1. Show that the eigenvalues of $A$ are real. Hint: try to multiply by the Hermitean transpose of the corresponding eigenvector.\r\n",
        "\r\n",
        "2. Show that if two eigenvalues are different, their eigenvectors are orthogonal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh5OFthnb-4f"
      },
      "source": [
        "# Problem 6 [2p]\n",
        "\n",
        "1D Gaussian manipulation for Kalman filters.\n",
        "\n",
        "A [1D Kalman filter](https://en.wikipedia.org/wiki/Kalman_filter) tracks the location of an object given imprecise measurements of its location. At its core it performs an update of the form:\n",
        "\n",
        "$$\n",
        "      p(x|m) = \\frac{p(m|x)p(x)}{p(m)} = \\frac{p(m|x)p(x)}{Z},\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $p(x|m)$ is the updated belief about the location,\n",
        "- $p(x) = \\mathcal{N}(\\mu=\\mu_x, \\sigma=\\sigma_x)$ is the belief about the location,\n",
        "- $p(m|x) = \\mathcal{N}(\\mu=x, \\sigma=\\sigma_m)$ is the noisy measurement, centered on the location of the object,\n",
        "- $Z = p(m) =\\int p(m|x)p(x) dx$ is a normalization constant not dependent on $x$.\n",
        "\n",
        "Compute $p(x|m)$.\n",
        "\n",
        "*Hint:* The product $\\mathcal{N}(x;\\mu_1, \\sigma_1)\\mathcal{N}(x;\\mu_2, \\sigma_2)$ resembles an unnormalized probability distribution, which one? Can you normalize it by computing the mean and standard deviation and fitting it to a known PDF?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZApJPB8vCbe"
      },
      "source": [
        "# Problem 7 [2bp]\r\n",
        "\r\n",
        "Let $X\\in \\R^{D\\times N}$ be a data matrix containing $N$ $D$-dimensional points. Let $Y\\in\\R^{1\\times N}$ be the targets.\r\n",
        "\r\n",
        "We have seen that the least squares problem\r\n",
        "$$\r\n",
        "\\min_{\\Theta} \\frac{1}{2}(\\Theta^T X - Y)(\\Theta^T X - Y)^T\r\n",
        "$$\r\n",
        "has a closed form solution\r\n",
        "$$\r\n",
        "\\Theta^T{}^* = Y X^T(X X^T)^{-1}\r\n",
        "$$\r\n",
        "Where $X^+ = X^T(X X^T)^{-1}$ is the right [Moore-Penrose pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) of $X$:\r\n",
        "$$\r\n",
        "\\begin{split}\r\n",
        "\\Theta^T X &\\approx Y \\\\\r\n",
        "\\Theta^T X X^+ &\\approx Y X^{+} \\\\\r\n",
        "\\Theta^T &= Y X^{+}\r\n",
        "\\end{split}\r\n",
        "$$\r\n",
        "\r\n",
        "The pseudoinverse also has another form (called a left inverse):\r\n",
        "$$\r\n",
        "X^+ = (X^T X)^{-1}X^T\r\n",
        "$$\r\n",
        "\r\n",
        "## P7.1 [0.5p]\r\n",
        "Say under which conditions the left and right pseudoinverses exist (when $X$ is a rectangular matrix only one pseudo-inverse exists). Give examples of machine learning problems that could be solved using each inverse.\r\n",
        "\r\n",
        "## P7.2 [1p]\r\n",
        "Derive the left inverse by solving the regularized least squares problem\r\n",
        "$$\r\n",
        "\\min_\\Theta \\sum_i(\\Theta^T x\\i - y\\i)^2 + \\lambda\\Theta^T\\Theta\r\n",
        "$$\r\n",
        "with artificially introduced variables $\\epsilon\\i$ and constraints $\\epsilon\\i = \\Theta^T x\\i - y\\i$, then see what happens when $\\lambda\\rightarrow 0$.\r\n",
        "\r\n",
        "## P7.3 [0.5p]\r\n",
        "Show that the above dual formulation allows using Kernel functions with linear regression. Express the optimal solution using a weighed avegage of data samples. How many \"support vectors\" there are?\r\n",
        "\r\n",
        "NB: some authors call the kernelized linear regression the \"Least-Squares SVM\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxVvXJ70-JqP"
      },
      "source": [
        "# Problem 8 (Bishop) [1p]\n",
        "\n",
        "Recall the SVM training problem\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "\\min_{w,b} & \\frac{1}{2}w^T w \\\\\n",
        "\\sjt & y\\i(w^Tx\\i+b) \\geq 1\\qquad \\textrm{for all } i.\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "Show that the solution for the maximum margin hyperplane doesn't change when the $1$\n",
        "on the right-hand side of the contraints is replaced by any $\\gamma>0$."
      ]
    }
  ]
}
